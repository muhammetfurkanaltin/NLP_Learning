{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b988fcc",
   "metadata": {},
   "source": [
    "### 7.1 Word Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ea8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - WE , kelimeleri sayisal vektörlerle ifade eden bir yöntemdir. \n",
    "# - Bu temsiller, kelimeler arasındaki anlamsal ilişkileri öğrenmeyi saglar.\n",
    "# - Aynı anlamda veya benzer anlamda kullanılan kelimeler vektor uzayında birbirine yakın olur.\n",
    "\n",
    "# CBOW = baglam kelimelere bakrak kelime tahmin etme\n",
    "# Skip-gram = CBOW tersine çalısıyor yani kelimeye bkılarak baglam kelimeleri tahmin ediyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce3a2c",
   "metadata": {},
   "source": [
    "### 7.2 RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4cf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Zaman serisi verisi \n",
    "# - Sekans Verisi ( Sıralı Veri )\n",
    "\n",
    "# Rnn' nin özellikleri \n",
    "    # - Zaman Boyutunda tekrar \n",
    "    # Sekans Verisi için uygun\n",
    "\n",
    "# RNN ile yapılan uygulamalar; \n",
    "#     - Dil Modelleme \n",
    "#     - Konuşma Tanıma\n",
    "#     - Makine Çevirisi\n",
    "#     - Görüntü Tanıma    \n",
    "#     - Metin üretimi\n",
    "#     -Duygu analizi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f0a0e",
   "metadata": {},
   "source": [
    "#### 7.3 RNN 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea89206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Solve Classification problem (Sentiment Analysis in NLP) with RNN (Deep Learning based Language Model)\n",
    "\n",
    "duygu analizi -> bir cumlenin etiketlenmesi (positive ve negative)\n",
    "restaurant yorumlari degerlendirme\n",
    "\"\"\"\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec # metin temsili\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# dataset\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Yemekler harikaydı, her şey taze ve lezzetliydi.\",\n",
    "        \"Garson çok ilgisizdi, siparişimi unuttular.\",\n",
    "        \"Tatlılar gerçekten çok güzeldi, bayıldım!\",\n",
    "        \"Yemekler soğuktu ve tadı hiç hoş değildi.\",\n",
    "        \"Atmosfer oldukça keyifliydi, tekrar geleceğim.\",\n",
    "        \"Fiyatlar biraz yüksekti ama yemekler güzeldi.\",\n",
    "        \"Servis kalitesi çok iyiydi, teşekkürler.\",\n",
    "        \"Yemek çok geç geldi, sabrım kalmadı.\",\n",
    "        \"Lezzetli bir akşam yemeği deneyimledik.\",\n",
    "        \"Bu restoranı asla tavsiye etmem, kötüydü.\",\n",
    "        \"Mekan çok hoştu, özellikle dekorasyonu.\",\n",
    "        \"Yemekler beklediğimden çok daha kötüydü.\",\n",
    "        \"Güzel bir akşam geçirdik, teşekkürler.\",\n",
    "        \"Yemekler fazlasıyla tuzlu geldi, hiç beğenmedim.\",\n",
    "        \"Kahvaltı muhteşemdi, her şeyi denemek istedim.\",\n",
    "        \"Fiyatlar oldukça makuldü, çok memnun kaldım.\",\n",
    "        \"Garsonlar çok yardımseverdi, teşekkürler.\",\n",
    "        \"Yemekler güzel ama servis biraz yavaştı.\",\n",
    "        \"Çocuklar için harika bir yer, çok eğlendiler.\",\n",
    "        \"Bir daha asla gitmeyeceğim, kötü bir deneyim yaşadım.\",\n",
    "        \"Mekanın atmosferi çok keyifliydi.\",\n",
    "        \"Yemeklerin tadı harikaydı, özellikle deniz ürünleri.\",\n",
    "        \"Şarap menüsü oldukça zengindi, beğendim.\",\n",
    "        \"Yemekler sıcak servis edilmedi, hayal kırıklığıydı.\",\n",
    "        \"Burgerleri gerçekten çok lezzetliydi.\",\n",
    "        \"Tatlıların fiyatı biraz yüksekti ama lezzetliydi.\",\n",
    "        \"Hizmet çok yavaştı ama yemekler fena değildi.\",\n",
    "        \"Gerçekten güzel bir akşam yemeği deneyimi yaşadık.\",\n",
    "        \"Sushi taze ve lezzetliydi, kesinlikle tavsiye ederim.\",\n",
    "        \"Garsonlar çok nazik ve yardımseverdi.\",\n",
    "        \"Hizmetin daha iyi olmasını beklerdim.\",\n",
    "        \"Kahvaltı menüsü oldukça zengindi, çok beğendim.\",\n",
    "        \"Yemekler çok lezzetliydi ama servis biraz yavaştı.\",\n",
    "        \"Fiyatlar oldukça makuldü, bu kadar iyi hizmete.\",\n",
    "        \"Mekan çok temizdi, bu benim için önemli.\",\n",
    "        \"Tatlıların çok şekerli olduğunu düşündüm.\",\n",
    "        \"Hizmet yavaş ama mekan güzeldi.\",\n",
    "        \"Yemeklerin lezzeti harikaydı ama porsiyonlar küçük.\",\n",
    "        \"Kendimi çok özel hissettim, teşekkürler.\",\n",
    "        \"Güzel bir akşam yemeği, tekrar geleceğim.\",\n",
    "        \"Çalışanlar çok güler yüzlüydü.\",\n",
    "        \"Pasta çok güzeldi, özellikle çikolatalı.\",\n",
    "        \"Biraz beklemek zorunda kaldık ama değdi.\",\n",
    "        \"Sadece fiyatlar biraz yüksekti ama lezzet buna değer.\",\n",
    "        \"Mekan oldukça kalabalıktı ama hizmet güzel.\",\n",
    "        \"Garsonlar çok nazik ama biraz daha hızlı olabilirdi.\",\n",
    "        \"Yemeklerin sunumu gerçekten etkileyiciydi.\",\n",
    "        \"Yemekler çok lezzetliydi ama garsonlar nazik değildi.\",\n",
    "        \"Çok güzel bir akşam yemeği deneyimi yaşadım.\",\n",
    "        \"Pasta siparişi verdim ama çok uzun sürdü.\"\n",
    "    ],\n",
    "    \"label\": [\n",
    "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\",\n",
    "        \"positive\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
    "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\",\n",
    "        \"positive\", \"positive\", \"positive\", \"negative\", \"negative\",\n",
    "        \"positive\", \"positive\", \"positive\", \"negative\", \"positive\",\n",
    "        \"negative\", \"positive\", \"positive\", \"positive\", \"positive\",\n",
    "        \"negative\", \"positive\", \"positive\", \"negative\", \"negative\",\n",
    "        \"negative\", \"positive\", \"positive\", \"positive\", \"positive\",\n",
    "        \"positive\", \"positive\", \"positive\", \"positive\", \"negative\",\n",
    "        \"negative\", \"positive\", \"positive\", \"positive\", \"negative\"\n",
    "\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cde31bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "23661aa0-bb53-40da-acf7-a541e92c3386",
       "rows": [
        [
         "0",
         "Yemekler harikaydı, her şey taze ve lezzetliydi.",
         "positive"
        ],
        [
         "1",
         "Garson çok ilgisizdi, siparişimi unuttular.",
         "negative"
        ],
        [
         "2",
         "Tatlılar gerçekten çok güzeldi, bayıldım!",
         "positive"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yemekler harikaydı, her şey taze ve lezzetliydi.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Garson çok ilgisizdi, siparişimi unuttular.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tatlılar gerçekten çok güzeldi, bayıldım!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text     label\n",
       "0  Yemekler harikaydı, her şey taze ve lezzetliydi.  positive\n",
       "1       Garson çok ilgisizdi, siparişimi unuttular.  negative\n",
       "2         Tatlılar gerçekten çok güzeldi, bayıldım!  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581c392",
   "metadata": {},
   "source": [
    "#### 7.4 RNN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7cfd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 8)\n"
     ]
    }
   ],
   "source": [
    "# metin temizleme ve preprocessing: tokenization, padding, label encoding, train test split\n",
    "\n",
    "# tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# padding process \n",
    "maxlen = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen = maxlen)\n",
    "print(X.shape)\n",
    "\n",
    "# label encoding \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3910bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metin temsili: word embedding: word2vec\n",
    "\n",
    "sentences = [text.split() for text in df[\"text\"]]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=50, window = 5, min_count=1) \n",
    "\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe205ef",
   "metadata": {},
   "source": [
    "#### 7.5 RNN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd24b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "20/20 [==============================] - 2s 18ms/step - loss: 0.6717 - accuracy: 0.6500 - val_loss: 0.7201 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.7000 - val_loss: 0.7957 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5934 - accuracy: 0.7000 - val_loss: 0.7425 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6045 - accuracy: 0.7000 - val_loss: 0.7602 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5965 - accuracy: 0.7000 - val_loss: 0.7901 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5856 - accuracy: 0.7000 - val_loss: 0.8081 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5632 - accuracy: 0.7000 - val_loss: 0.7644 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5349 - accuracy: 0.7250 - val_loss: 0.8716 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5125 - accuracy: 0.7250 - val_loss: 0.8965 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4637 - accuracy: 0.7500 - val_loss: 0.9018 - val_accuracy: 0.4000\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.9018 - accuracy: 0.4000\n",
      "Test loss: 0.9017836451530457\n",
      "Test Accuracy: 0.4000000059604645\n"
     ]
    }
   ],
   "source": [
    "# modelling: build, train ve test rnn modeli \n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "# embedding\n",
    "model.add(Embedding(input_dim = len(word_index) + 1, output_dim = embedding_dim, weights = [embedding_matrix], input_length=maxlen, trainable = False))\n",
    "\n",
    "# RNN layer\n",
    "model.add(SimpleRNN(50, return_sequences = False))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size = 2, validation_data=(X_test, y_test))\n",
    "\n",
    "# evaluate rnn model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bd5f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 278ms/step\n",
      "Result: positive\n"
     ]
    }
   ],
   "source": [
    "# cumle siniflandirma calismasi\n",
    "def classify_sentence(sentence):\n",
    "    \n",
    "    seq = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_seq = pad_sequences(seq, maxlen = maxlen) \n",
    "    \n",
    "    prediction = model.predict(padded_seq)\n",
    "    \n",
    "    predicted_class = (prediction > 0.5).astype(int)\n",
    "    label = \"positive\" if predicted_class[0][0] == 1 else \"negative\"\n",
    "    \n",
    "    return label\n",
    "\n",
    "sentence = \"Restaurant çok temizdi ve yemekler çok güzeldi\"\n",
    "\n",
    "result = classify_sentence(sentence)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a2a70",
   "metadata": {},
   "source": [
    "### 7.6 LSTM ( Long Short-Term Memory )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574e3a7",
   "metadata": {},
   "source": [
    "![ScreenS/7.6_LSTM.PNG](ScreenS/7.6_LSTM.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec405c2",
   "metadata": {},
   "source": [
    "![ScreenS/RNNvsLSTM.PNG](ScreenS/RNNvsLSTM.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb17c879",
   "metadata": {},
   "source": [
    "#### 7.7 LSTM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d103b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "metin uretimi\n",
    "lstm train with text data\n",
    "text data = gpt ile olustur\n",
    "\"\"\"\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# egitim verisi chatgpt ile olustur\n",
    "texts = [\n",
    "    \"Bugün hava çok güzel, dışarıda yürüyüş yapmayı düşünüyorum.\",\n",
    "    \"Kitap okumak beni gerçekten mutlu ediyor.\",\n",
    "    \"Kahve içmeden güne başlamak zor geliyor.\",\n",
    "    \"Akşam yemeğinde pizza yemeyi planlıyorum.\",\n",
    "    \"Sinemaya gitmek her zaman keyifli bir aktivite.\",\n",
    "    \"Sabah koşusu bana enerji veriyor.\",\n",
    "    \"Yeni bir dil öğrenmek beni heyecanlandırıyor.\",\n",
    "    \"Hafta sonu arkadaşlarımla buluşacağım.\",\n",
    "    \"Doğada vakit geçirmek beni rahatlatıyor.\",\n",
    "    \"Müzik dinlemek beni motive ediyor.\",\n",
    "    \"Yarın önemli bir toplantım var, çok heyecanlıyım.\",\n",
    "    \"Yeni bir kitap aldım, bu hafta sonu okumayı planlıyorum.\",\n",
    "    \"Akşam dışarıda yemek yemeyi düşünüyorum.\",\n",
    "    \"Bu sabah yoga yaparak güne başladım.\",\n",
    "    \"Sıcak bir çay içmek beni her zaman rahatlatır.\",\n",
    "    \"Şehirde yeni bir restoran açılmış, gitmek istiyorum.\",\n",
    "    \"Uzun zamandır bisiklete binmiyordum, bugün binmeyi planlıyorum.\",\n",
    "    \"Bahçede çiçekler açmış, harika görünüyor.\",\n",
    "    \"Bu hafta çok yoğundum, biraz dinlenmeye ihtiyacım var.\",\n",
    "    \"Yeni bir diziye başladım, oldukça sürükleyici.\",\n",
    "    \"Çalışmalarımda daha verimli olmak istiyorum.\",\n",
    "    \"Bugün alışveriş yapmam gerekiyor, eksiklerim var.\",\n",
    "    \"Sabahları yürüyüş yapmak benim için iyi bir alışkanlık oldu.\",\n",
    "    \"Yeni tarifler denemek mutfakta zaman geçirmemi keyifli kılıyor.\",\n",
    "    \"Uzun zamandır görmediğim bir arkadaşımla buluşacağım.\",\n",
    "    \"Ders çalışırken sessiz bir ortamda olmak bana daha çok odaklanmamı sağlıyor.\",\n",
    "    \"Bugün işlerimi erken bitirip biraz dinleneceğim.\",\n",
    "    \"Hafta sonu piknik yapmayı düşünüyorum.\",\n",
    "    \"Yaz tatili için plan yapmaya başladım.\",\n",
    "    \"Bugün kendime biraz zaman ayırıp film izlemeyi düşünüyorum.\",\n",
    "    \"Kış aylarını seviyorum, çünkü sıcak çikolata içmeyi çok seviyorum.\",\n",
    "    \"Telefonumun şarjı bitmek üzere, hemen şarja takmam lazım.\",\n",
    "    \"İşimle ilgili yeni projeler üzerinde çalışıyorum.\",\n",
    "    \"Bugün spora gitmek beni gerçekten zorlayacak.\",\n",
    "    \"Tiyatroya gitmeyi uzun zamandır planlıyordum, bu hafta gitmeyi düşünüyorum.\",\n",
    "    \"Yeni müzikler keşfetmek her zaman ilgimi çekiyor.\",\n",
    "    \"Yarın sabah erkenden uyanmam gerekiyor.\",\n",
    "    \"Dışarıda çok güzel bir hava var, belki biraz bisiklet sürerim.\",\n",
    "    \"Bahçeye birkaç yeni bitki ekmeyi düşünüyorum.\",\n",
    "    \"Kütüphaneye gidip yeni kitaplar bakacağım.\",\n",
    "    \"Evdeki işlerimi bitirip bir kahve molası vereceğim.\",\n",
    "    \"Gelecek hafta için bir tatil planı yapıyorum.\",\n",
    "    \"Bugün işyerinde toplantılar arka arkaya sıralanmış.\",\n",
    "    \"Yeni bir hobi edinmek istiyorum, belki resim yapmayı deneyebilirim.\",\n",
    "    \"Bu hafta spor salonuna düzenli olarak gitmeye karar verdim.\",\n",
    "    \"Sabahları meditasyon yapmak beni çok rahatlatıyor.\",\n",
    "    \"Ailecek bu akşam film gecesi yapacağız.\",\n",
    "    \"Yaz mevsimi yaklaşırken, deniz tatili planları yapıyorum.\",\n",
    "    \"Bugün işim erken biterse bir kitap okumayı planlıyorum.\",\n",
    "    \"Yeni tarifler denemek mutfakta daha fazla zaman geçirmemi sağlıyor.\",\n",
    "    \"Bugün alışveriş merkezine gidip ihtiyaçlarımı tamamlayacağım.\",\n",
    "    \"Gelecek hafta arkadaşlarımla bir doğa yürüyüşüne çıkmayı düşünüyoruz.\",\n",
    "    \"Yatmadan önce biraz müzik dinlemek rahatlamamı sağlıyor.\",\n",
    "    \"Bugün biraz daha geç kalktım ve kendime vakit ayırdım.\",\n",
    "    \"Yarın işim erken biterse yeni bir diziye başlayabilirim.\",\n",
    "    \"Mutfakta yeni bir tatlı denemek istiyorum.\",\n",
    "    \"Bu hafta sonu şehir dışında bir yere gitmeyi planlıyoruz.\",\n",
    "    \"Yeni bir fotoğraf makinesi aldım, hafta sonu denemek için sabırsızlanıyorum.\",\n",
    "    \"Sabahları güneşin doğuşunu izlemek bana enerji veriyor.\",\n",
    "    \"İş yoğunluğu arasında kısa bir mola vermek bana iyi geliyor.\",\n",
    "    \"Akşam yemeği için dışarıda bir yer arıyorum.\",\n",
    "    \"Bugün havalar biraz soğudu, kalın giyinmem gerek.\",\n",
    "    \"Yeni bir film izlemek için sinemaya gitmeyi düşünüyorum.\",\n",
    "    \"Bu hafta sonu evde dinlenip enerji toplamayı planlıyorum.\",\n",
    "    \"İşimle ilgili bir sunum hazırlamam gerekiyor.\",\n",
    "    \"Yaz tatili için sahil kasabasına gitmek istiyorum.\",\n",
    "    \"Dışarıda yağmur yağıyor, tam kitap okuma havası.\",\n",
    "    \"Sabah kahvaltısında yeni bir şeyler denemeyi seviyorum.\",\n",
    "    \"Bugün ofiste biraz yoğun bir gün geçirdim.\",\n",
    "    \"Kış ayları yaklaşıyor, dolabımı yenilemem gerek.\",\n",
    "    \"Yeni bir spor dalı denemek istiyorum, belki yoga.\",\n",
    "    \"Hafta sonu için arkadaşlarımla bir etkinlik planlıyoruz.\",\n",
    "    \"Bu akşam evde kendime biraz zaman ayırıp dinleneceğim.\",\n",
    "    \"Yeni bir bilgisayar oyunu keşfettim, oldukça eğlenceli.\",\n",
    "    \"Sabahları kahve içmeden kendime gelemiyorum.\",\n",
    "    \"Yarın sabah erkenden uyanıp yürüyüş yapmayı planlıyorum.\",\n",
    "    \"Hafta sonu sahilde vakit geçirmek harika olurdu.\",\n",
    "    \"Bugün birkaç yeni tarif denedim, oldukça lezzetli oldu.\",\n",
    "    \"Bu hafta işte oldukça yoğun geçiyor.\",\n",
    "    \"Yarın için önemli bir randevum var.\",\n",
    "    \"Bugün dışarıda yemek yemeyi planlıyorum.\",\n",
    "    \"Evde temizlik yapmak için güzel bir gün.\",\n",
    "    \"Bu sabah spora gitmek beni zorlayacak gibi görünüyor.\",\n",
    "    \"Hafta sonu şehir dışına kısa bir kaçamak yapmayı düşünüyoruz.\",\n",
    "    \"Sabahları kahvaltı yapmadan güne başlamam mümkün değil.\",\n",
    "    \"Yeni bir hobi edinmek bana iyi gelebilir.\",\n",
    "    \"Kitap okumak beni her zaman başka bir dünyaya götürüyor.\",\n",
    "    \"Bugün çok verimli bir gün geçirdim.\",\n",
    "    \"Yarın yeni bir projeye başlamayı planlıyorum.\",\n",
    "    \"Bu hafta sonu ailecek vakit geçirmeyi planlıyoruz.\",\n",
    "    \"Yeni bir film izledim, oldukça etkileyiciydi.\",\n",
    "    \"Sabahları yürüyüş yapmak bana enerji veriyor.\",\n",
    "    \"Dışarıda çok güzel bir hava var, belki biraz yürüyüş yaparım.\",\n",
    "    \"Akşam yemeği için bir şeyler hazırlamam gerekiyor.\",\n",
    "    \"Yarın arkadaşlarımla bir araya geleceğim, sabırsızlanıyorum.\",\n",
    "    \"Bu hafta biraz daha fazla dinlenmeye ihtiyacım var.\",\n",
    "    \"Bugün işlerimi erken bitirip biraz kitap okuyacağım.\",\n",
    "    \"Yeni bir müzik albümü keşfetmek beni mutlu ediyor.\",\n",
    "    \"Doğada vakit geçirmek beni her zaman sakinleştiriyor.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365bee33",
   "metadata": {},
   "source": [
    "#### 7.8 LSTM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab05860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metin temizleme ve preprocessing: tokenization, padding, label encoding\n",
    "\n",
    "# tokenization \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts) # metinler üzerindeki kelime frekanslarini ogren\n",
    "total_words = len(tokenizer.word_index) + 1 # toplam kelime sayisi\n",
    "\n",
    "# n-gram dizileri olustur ve padding uygula \n",
    "input_sequences = []\n",
    "for text in texts: \n",
    "    # metinleri kelime dizilerine cevir\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "    # her metin icin n-gram dizisi olustur\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "# en uzun diziyi bulalım, tum dizileri ayni uzunluga getirelim\n",
    "max_sequence_length = max(len(x) for x in input_sequences)\n",
    "\n",
    "# dizilere diziyi bulalım, tum dizileri aynı uzunluga getirelim\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# X ( girdi ) ve y ( hedef )\n",
    "# \n",
    "X = input_sequences[:,:-1] # son kelimeyi almayacagiz\n",
    "y = input_sequences[:,-1] # son kelimeyi alacagiz\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) # one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7be435",
   "metadata": {},
   "source": [
    "#### 7.9 LSTM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50e5d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "19/19 [==============================] - 3s 9ms/step - loss: 5.7249 - accuracy: 0.0436\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 5.5501 - accuracy: 0.0688\n",
      "Epoch 3/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 5.3372 - accuracy: 0.0688\n",
      "Epoch 4/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 5.2866 - accuracy: 0.0688\n",
      "Epoch 5/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 5.2534 - accuracy: 0.0688\n",
      "Epoch 6/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 5.2108 - accuracy: 0.0688\n",
      "Epoch 7/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 5.1848 - accuracy: 0.0688\n",
      "Epoch 8/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 5.1481 - accuracy: 0.0688\n",
      "Epoch 9/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 5.0673 - accuracy: 0.0688\n",
      "Epoch 10/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 4.9810 - accuracy: 0.0721\n",
      "Epoch 11/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 4.8900 - accuracy: 0.0755\n",
      "Epoch 12/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 4.7919 - accuracy: 0.0789\n",
      "Epoch 13/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 4.7099 - accuracy: 0.0839\n",
      "Epoch 14/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 4.5879 - accuracy: 0.0839\n",
      "Epoch 15/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 4.5044 - accuracy: 0.0973\n",
      "Epoch 16/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 4.4104 - accuracy: 0.1040\n",
      "Epoch 17/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 4.2942 - accuracy: 0.1242\n",
      "Epoch 18/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 4.1851 - accuracy: 0.1443\n",
      "Epoch 19/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 4.0806 - accuracy: 0.1577\n",
      "Epoch 20/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 3.9725 - accuracy: 0.1896\n",
      "Epoch 21/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 3.8462 - accuracy: 0.2064\n",
      "Epoch 22/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 3.7348 - accuracy: 0.2215\n",
      "Epoch 23/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 3.6168 - accuracy: 0.2366\n",
      "Epoch 24/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 3.5034 - accuracy: 0.2601\n",
      "Epoch 25/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 3.3885 - accuracy: 0.2735\n",
      "Epoch 26/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 3.2763 - accuracy: 0.3037\n",
      "Epoch 27/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 3.1740 - accuracy: 0.3188\n",
      "Epoch 28/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 3.0631 - accuracy: 0.3440\n",
      "Epoch 29/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 2.9524 - accuracy: 0.3641\n",
      "Epoch 30/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 2.8521 - accuracy: 0.4044\n",
      "Epoch 31/100\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 2.7520 - accuracy: 0.4195\n",
      "Epoch 32/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 2.6559 - accuracy: 0.4597\n",
      "Epoch 33/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 2.5689 - accuracy: 0.4631\n",
      "Epoch 34/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 2.4743 - accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 2.3983 - accuracy: 0.5419\n",
      "Epoch 36/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 2.3089 - accuracy: 0.5470\n",
      "Epoch 37/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 2.2285 - accuracy: 0.5755\n",
      "Epoch 38/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 2.1514 - accuracy: 0.5956\n",
      "Epoch 39/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 2.0731 - accuracy: 0.6275\n",
      "Epoch 40/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 2.0003 - accuracy: 0.6376\n",
      "Epoch 41/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.9295 - accuracy: 0.6493\n",
      "Epoch 42/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 1.8625 - accuracy: 0.6611\n",
      "Epoch 43/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 1.7989 - accuracy: 0.6913\n",
      "Epoch 44/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 1.7370 - accuracy: 0.6930\n",
      "Epoch 45/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.6713 - accuracy: 0.7114\n",
      "Epoch 46/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.6228 - accuracy: 0.7181\n",
      "Epoch 47/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.5647 - accuracy: 0.7198\n",
      "Epoch 48/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.5111 - accuracy: 0.7299\n",
      "Epoch 49/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.4619 - accuracy: 0.7366\n",
      "Epoch 50/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 1.4182 - accuracy: 0.7483\n",
      "Epoch 51/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.3652 - accuracy: 0.7685\n",
      "Epoch 52/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.3180 - accuracy: 0.7685\n",
      "Epoch 53/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.2787 - accuracy: 0.7718\n",
      "Epoch 54/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.2337 - accuracy: 0.7785\n",
      "Epoch 55/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.1938 - accuracy: 0.7987\n",
      "Epoch 56/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.1625 - accuracy: 0.7936\n",
      "Epoch 57/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.1285 - accuracy: 0.8037\n",
      "Epoch 58/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.0878 - accuracy: 0.8171\n",
      "Epoch 59/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.0582 - accuracy: 0.8188\n",
      "Epoch 60/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 1.0248 - accuracy: 0.8305\n",
      "Epoch 61/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.9955 - accuracy: 0.8272\n",
      "Epoch 62/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.9660 - accuracy: 0.8356\n",
      "Epoch 63/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.9392 - accuracy: 0.8372\n",
      "Epoch 64/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.9150 - accuracy: 0.8456\n",
      "Epoch 65/100\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.8860 - accuracy: 0.8523\n",
      "Epoch 66/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.8632 - accuracy: 0.8456\n",
      "Epoch 67/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.8420 - accuracy: 0.8473\n",
      "Epoch 68/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.8154 - accuracy: 0.8574\n",
      "Epoch 69/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7981 - accuracy: 0.8591\n",
      "Epoch 70/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7754 - accuracy: 0.8574\n",
      "Epoch 71/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7582 - accuracy: 0.8574\n",
      "Epoch 72/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7384 - accuracy: 0.8691\n",
      "Epoch 73/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.7211 - accuracy: 0.8708\n",
      "Epoch 74/100\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 0.7050 - accuracy: 0.8641\n",
      "Epoch 75/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6892 - accuracy: 0.8607\n",
      "Epoch 76/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6747 - accuracy: 0.8641\n",
      "Epoch 77/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6615 - accuracy: 0.8725\n",
      "Epoch 78/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6433 - accuracy: 0.8725\n",
      "Epoch 79/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.6285 - accuracy: 0.8658\n",
      "Epoch 80/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6172 - accuracy: 0.8725\n",
      "Epoch 81/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.6039 - accuracy: 0.8641\n",
      "Epoch 82/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.5920 - accuracy: 0.8691\n",
      "Epoch 83/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5841 - accuracy: 0.8641\n",
      "Epoch 84/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5732 - accuracy: 0.8691\n",
      "Epoch 85/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5636 - accuracy: 0.8691\n",
      "Epoch 86/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5515 - accuracy: 0.8674\n",
      "Epoch 87/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5419 - accuracy: 0.8708\n",
      "Epoch 88/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5324 - accuracy: 0.8658\n",
      "Epoch 89/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5233 - accuracy: 0.8742\n",
      "Epoch 90/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5181 - accuracy: 0.8742\n",
      "Epoch 91/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5099 - accuracy: 0.8742\n",
      "Epoch 92/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.5031 - accuracy: 0.8658\n",
      "Epoch 93/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4925 - accuracy: 0.8758\n",
      "Epoch 94/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4875 - accuracy: 0.8758\n",
      "Epoch 95/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4800 - accuracy: 0.8792\n",
      "Epoch 96/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4720 - accuracy: 0.8775\n",
      "Epoch 97/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4663 - accuracy: 0.8708\n",
      "Epoch 98/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4630 - accuracy: 0.8658\n",
      "Epoch 99/100\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.4542 - accuracy: 0.8758\n",
      "Epoch 100/100\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4505 - accuracy: 0.8725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13b31dc61d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM modeli olustur, compile, train ve evaluate \n",
    "model = Sequential()\n",
    "\n",
    "# embedding \n",
    "model.add(Embedding(total_words, 50, input_length = X.shape[1]))\n",
    "\n",
    "# lstm \n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "\n",
    "# output\n",
    "model.add(Dense(total_words, activation = \"softmax\"))\n",
    "\n",
    "# model compile \n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "# model train\n",
    "model.fit(X, y, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5b0cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sinemaya gitmek her zaman keyifli bir aktivite götürüyor\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "def generate_text(seed_text, next_words):\n",
    "    for _ in range(next_words):\n",
    "        # girdi metnini sayisal verilere donustur\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "        # padding \n",
    "        token_list = pad_sequences([token_list], maxlen = max_sequence_length-1, padding=\"pre\")\n",
    "\n",
    "        # prediction \n",
    "        predicted_probabilities = model.predict(token_list, verbose = 0)\n",
    "\n",
    "        # en yuksek olasiliga sahip kelimenin indexini bul \n",
    "        predicted_word_index = np.argmax(predicted_probabilities, axis = 1)\n",
    "\n",
    "        # tokenizer ile kelime index inden asil kelimeyi bul \n",
    "        predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
    "\n",
    "        # tahmin edilen kelimeyi seed_text e ekleyelim\n",
    "        seed_text = seed_text + \" \" + predicted_word\n",
    "    return seed_text\n",
    "seed_text = \"sinemaya gitmek\"\n",
    "print(generate_text(seed_text, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ec13a",
   "metadata": {},
   "source": [
    "### 7.10 Transformers Model (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cdeb15",
   "metadata": {},
   "source": [
    "#### 7.11 GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a069c21",
   "metadata": {},
   "source": [
    "![ScreenS/7.11_Transformers.PNG](ScreenS/7.11_Transformers.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a00789",
   "metadata": {},
   "source": [
    "#### 7.12 LLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4555a",
   "metadata": {},
   "source": [
    "- Transformers tabanlı \n",
    "- hafif ve verimli \n",
    "- daha kücük ve daha hızlı\n",
    "\n",
    "![ScreenS/7.12_LLAMA.PNG](ScreenS/7.12_LLAMA.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d415c91",
   "metadata": {},
   "source": [
    "![ScreenS/7.12_karsılastırma.PNG](ScreenS/7.12_karsılastırma.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274d80b",
   "metadata": {},
   "source": [
    "#### 7.13 GPT ve LLAMA modelleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b0ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mfurk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mfurk\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afternoon,  I was sitting in the kitchen, and I was thinking about the next day\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# modelin tanimlanmasi \n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# tokenizer tanimla ve model olusturma\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# metin uretimi icin gerekli olan baslagıc text i \n",
    "text = \"Afternoon, \"\n",
    "\n",
    "# tokenization \n",
    "inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# metin uretimi gerceklestirelim\n",
    "outputs = model.generate(inputs, max_length = 20 )\n",
    "\n",
    "# modelin urettigi tokenleri okunabilir hale getirmemiz lazım \n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True) # ozel tokenleri göstermemek icin \"True\" dedik\n",
    "\n",
    "# uretilen metni print ettirelim \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da7160",
   "metadata": {},
   "source": [
    "#### LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563fe915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# modelin tanimlanmasi \n",
    "model_name_llama = \"huggyllama/llama-7b\" #llama\n",
    "\n",
    "# tokenizer tanimla ve model olusturma\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama) #llama\n",
    "\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(model_name_llama) #llama\n",
    "\n",
    "# metin uretimi icin gerekli olan baslagıc text i \n",
    "text = \"Afternoon, \"\n",
    "\n",
    "# tokenization \n",
    "inputs_llama = tokenizer_llama.encode(text, return_tensors=\"pt\") #llama\n",
    "\n",
    "# metin uretimi gerceklestirelim\n",
    "outputs_llama = model_llama.generate(inputs_llama, max_length = 20 ) #llama\n",
    "\n",
    "# modelin urettigi tokenleri okunabilir hale getirmemiz lazım \n",
    "generated_text_llama = tokenizer_llama.decode(outputs[0], skip_special_tokens=True) #llama\n",
    "\n",
    "# uretilen metni print ettirelim \n",
    "print(generated_text_llama) #llama"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
