{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e731b3dd",
   "metadata": {},
   "source": [
    "### 8.3 Metin Sınıflandırma Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f6d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "spam veri seti -> spam ve ham -> binanry classification with Decision Tree\n",
    "\"\"\"\n",
    "\n",
    "# import libraries \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "# veri yükle \n",
    "data = pd.read_csv(\"datasets/metin_siniflandirma_spam_veri_seti.csv\", encoding=\"latin-1\")\n",
    "data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\n",
    "data.columns = ['label', 'text']\n",
    "\n",
    "# EDA : Kesifsel veri analizi : missing value\n",
    "\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c9de5",
   "metadata": {},
   "source": [
    "#### 8.4 Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a6d4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text cleaning and preprocessing : ozel karakterler, lowercse, tokenization , stopwords, lemmatization\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords') # cok kullanılan ve anlam tasimayan sozcukler imetin icersinden cıkartalım\n",
    "nltk.download('wordnet') # lemma bulmak icin gerekli olan veriseti\n",
    "nltk.download('omw-1.4') # wordnete ait farkli dillerin kelime anlamlarını iceren bir veri seti \n",
    "\n",
    "import re \n",
    "from nltk.corpus import stopwords # stopwords lerden kurtulmak icin \n",
    "from nltk.stem import WordNetLemmatizer # lemmatization\n",
    "\n",
    "text = list(data.text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "    r = re.sub(\"[^a-zA-Z]\", \" \", text[i]) # metin icersinde harf olmayan tum karakterleri boslukla degistir\n",
    "    r = r.lower() # tum harfleri kucuk harfe cevir\n",
    "    r = r.split() # kelimeleri ayir\n",
    "    r = [word for word in r if word not in stopwords.words('english')] # stopwordslerden kurtul\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "    r = \" \".join(r)\n",
    "    corpus.append(r)\n",
    "data['text2'] = corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b8ba9",
   "metadata": {},
   "source": [
    "#### 8.5 Part 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f75d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 96.77%\n"
     ]
    }
   ],
   "source": [
    "# model training and evaluation\n",
    "\n",
    "X = data['text2']\n",
    "y = data['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "# feature extraction : BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "# classifier training : model training and evaluation \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train_cv, y_train)\n",
    "\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "# prediction\n",
    "prediction = dt.predict(X_test_cv)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "c_matrix = confusion_matrix(y_test, prediction)\n",
    "\n",
    "acc = 100*(sum(sum(c_matrix)) - c_matrix[1,0] - c_matrix[0 ,1]) / sum(sum(c_matrix))\n",
    "print(f\"accuracy : {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0a947",
   "metadata": {},
   "source": [
    "### 8.6 Varlık ismi Tanıma ( Named Entity Recognition ( NER ) ) Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350351f",
   "metadata": {},
   "source": [
    "![ScreenS/8.6_NER.PNG](ScreenS/8.6_NER.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebe975",
   "metadata": {},
   "source": [
    "#### 8.7 NER Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6576fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 23.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 27.7 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Alice PERSON\n",
      "Amazon ORG\n",
      "London GPE\n",
      "the British Museum ORG\n",
      "last weekend DATE\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "warlik ismi tanima  mein ( cumle ) -> metin icersinde bulunan vrlik isimlerini tanımla \n",
    "\"\"\"\n",
    "\n",
    "%pip install spacy -q\n",
    "\n",
    "import pandas as pd \n",
    "import spacy \n",
    "\n",
    "# spacy modeli ile varlik ismi tanimla \n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\") # spacy kutuphanesi ingilizce dil modeli \n",
    "\n",
    "content = \"Alice works at Amazon and lives in London. She visited the British Museum last weekend.\"\n",
    "\n",
    "doc = nlp(content) # bu islem metindeki varlıklari ( entities ) analiz eder \n",
    "\n",
    "for ent in doc.ents: \n",
    "    # ent.text  varlik ismi\n",
    "    # ent.start_char ve ent.end_char : varligi metindekki baslangic ve bitis karakterler \n",
    "    # print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    print(ent.text,ent.label_)\n",
    "\n",
    "# ent.lemma_ : varligin kok hali \n",
    "entites = [(ent.text, ent.label_, ent.lemma_) for ent in doc.ents]\n",
    "\n",
    "# varlik listesindi pandas df e cevir \n",
    "df = pd.DataFrame(entites, columns=[\"text\", \"type\", \"lemma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c8c12",
   "metadata": {},
   "source": [
    "### 8.8 Morfolojik Analiz ( Morphological Analysis ) Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f19f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örn : \"kitaplar \" kelimesinin kökünü \" kitap \" ve ekinin \" -lar \"\n",
    "#   oldugunu belirleyerek kelimenin cogul oldugunu tespit etmek. \n",
    "\n",
    "# kullanim alanları ; \n",
    "# Dil ögrenme araçlari\n",
    "# Doğal dil işleme\n",
    "# Otomatik çeviri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d850f",
   "metadata": {},
   "source": [
    "#### 8.9 Morphological Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adfeaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I\n",
      "Lemma: I\n",
      "POS: PRON\n",
      "Tag: PRP\n",
      "Dependency: nsubj\n",
      "Shape: X\n",
      "Is alpha: True\n",
      "Is stop: True\n",
      "Morfoloji: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "Is plural: False\n",
      "\n",
      "Text: go\n",
      "Lemma: go\n",
      "POS: VERB\n",
      "Tag: VBP\n",
      "Dependency: ROOT\n",
      "Shape: xx\n",
      "Is alpha: True\n",
      "Is stop: True\n",
      "Morfoloji: Tense=Pres|VerbForm=Fin\n",
      "Is plural: False\n",
      "\n",
      "Text: to\n",
      "Lemma: to\n",
      "POS: ADP\n",
      "Tag: IN\n",
      "Dependency: prep\n",
      "Shape: xx\n",
      "Is alpha: True\n",
      "Is stop: True\n",
      "Morfoloji: \n",
      "Is plural: False\n",
      "\n",
      "Text: schools\n",
      "Lemma: school\n",
      "POS: NOUN\n",
      "Tag: NNS\n",
      "Dependency: pobj\n",
      "Shape: xxxx\n",
      "Is alpha: True\n",
      "Is stop: False\n",
      "Morfoloji: Number=Plur\n",
      "Is plural: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# incelenecek olan kelime yada kelimeler\n",
    "\n",
    "word = \"I go to schools\"\n",
    "\n",
    "# kelimeyi nlp isleminden gecir\n",
    "doc = nlp(word)\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    print(f\"Text: {token.text}\")            # kelimenin kendisi\n",
    "    print(f\"Lemma: {token.lemma_}\")         # kelimenin kok hali\n",
    "    print(f\"POS: {token.pos_}\")             # kelimenin dilbilgisel ozelligi\n",
    "    print(f\"Tag: {token.tag_}\")             # kelimenin detayli dilbilgisel ozelligi\n",
    "    print(f\"Dependency: {token.dep_}\")      # kelimenin rolu \n",
    "    print(f\"Shape: {token.shape_}\")         # karakter yapisi\n",
    "    print(f\"Is alpha: {token.is_alpha}\")    # kelimenin yalnizca alfabetik karakterlerden olusup olusmadigini kontrol eder\n",
    "    print(f\"Is stop: {token.is_stop}\")      # kelimenin stop words olup olmadigi\n",
    "    print(f\"Morfoloji: {token.morph}\")      # kelimenin morfolojik ozelliklerini verir \n",
    "    print(f\"Is plural: {'Number=Plur' in token.morph}\") # kelimenin cogul olup olmadigi\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e1bee",
   "metadata": {},
   "source": [
    "### 8.10 Metin Parçası Etiketleme ( Part of Speech ( POS )) Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cc15d",
   "metadata": {},
   "source": [
    "![ScreenS/8.10_POS.PNG](ScreenS/8.10_POS.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93265e",
   "metadata": {},
   "source": [
    "#### POS Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0dea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What PRON\n",
      "is AUX\n",
      "the DET\n",
      "weather NOUN\n",
      "like ADP\n",
      "today NOUN\n",
      "or CCONJ\n",
      "tomorrow NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence1 = \"What is the weather like today or tomorrow\"\n",
    "doc1 = nlp(sentence1)\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52396166",
   "metadata": {},
   "source": [
    "### Kelime Anlamı Belirsizlği Giderme ( Word sense Disambiguation ) Part 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c380e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WSD, bir kelimenin farklı anlamları arasında dogru olanı baglama göre seçme işlemidir \n",
    "# kullanım alanlari ; \n",
    "#  makine çevirisi \n",
    "#  arama motorlari \n",
    "#  dogal dil işlmee "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4261617",
   "metadata": {},
   "source": [
    "#### WSD Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d03b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading own-1.4: Package 'own-1.4' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "# gerekli nltk paketlerini indir\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"own-1.4\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2131e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumle:  I go to the bank to deposit money\n",
      "Word: bank\n",
      "Sense: a container (usually with a slot in the top) for keeping money at home\n"
     ]
    }
   ],
   "source": [
    "# ilk cumle\n",
    "s1 = \" I go to the bank to deposit money\"\n",
    "w1 = \"bank\"\n",
    "\n",
    "sense1 = lesk(nltk.word_tokenize(s1), w1)\n",
    "print(f\"Cumle: {s1}\")\n",
    "print(f\"Word: {w1}\")\n",
    "print(f\"Sense: {sense1.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce462ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumle: The river bank is flooded after the heavy rain\n",
      "Word: bank\n",
      "Sense: a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n"
     ]
    }
   ],
   "source": [
    "s2 = \"The river bank is flooded after the heavy rain\"\n",
    "w2 = \"bank\"\n",
    "sense2 = lesk(nltk.word_tokenize(s2), w2)\n",
    "\n",
    "print(f\"Cumle: {s2}\")\n",
    "print(f\"Word: {w2}\")\n",
    "print(f\"Sense: {sense2.definition()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a2411",
   "metadata": {},
   "source": [
    "#### WSD Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import simple_lesk, adapted_lesk, cosine_lesk\n",
    "\n",
    "# ornek cumle\n",
    "sentences = [\n",
    "    \"I go to the bank to deposit money\",\n",
    "    \"The river bank was flooded after the heavy rain\"]\n",
    "\n",
    "word = \"bank\"\n",
    "\n",
    "for s in sentences:\n",
    "    \n",
    "    print(f\"Sentence: {s}\")\n",
    "    \n",
    "    sense_simple_lesk = simple_lesk(s, word)\n",
    "    print(f\"Sense simple: {sense_simple_lesk.definition()}\")\n",
    "    \n",
    "    sense_adapted_lesk = adapted_lesk(s, word)\n",
    "    print(f\"Sense adapted: {sense_adapted_lesk.definition()}\")\n",
    "    \n",
    "    sense_cosine_lesk = cosine_lesk(s, word)\n",
    "    print(f\"Sense cosine: {sense_cosine_lesk.definition()}\")\n",
    "    \n",
    "\"\"\"\n",
    "Sentence: I go to the bank to deposit money (banka)\n",
    "Sense simple: a financial institution that accepts deposits and channels the money into lending activities\n",
    "Sense adapted: a financial institution that accepts deposits and channels the money into lending activities\n",
    "Sense cosine: a container (usually with a slot in the top) for keeping money at home\n",
    "\n",
    "Sentence: The river bank was flooded after the heavy rain\n",
    "Sense simple: sloping land (especially the slope beside a body of water)\n",
    "Sense adapted: sloping land (especially the slope beside a body of water)\n",
    "Sense cosine: a supply or stock held in reserve for future use (especially in emergencies)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b83d3",
   "metadata": {},
   "source": [
    "### Duygu Analizi ( Sentiment Analysis ) Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32333a43",
   "metadata": {},
   "source": [
    "#### SA Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5586859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mfurk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "binanry classification problemi \n",
    "\"\"\"\n",
    "# import libraries \n",
    "import pandas as pd \n",
    "import nltk \n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "df = pd.read_csv('datasets/duygu_analizi_amazon_veri_seti.csv')\n",
    "\n",
    "# text cleaning ve presprocessing \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_preprocess_data(text):\n",
    "\n",
    "    # tokenize \n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # join words\n",
    "    processed_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    return processed_text\n",
    "df[\"reviewText2\"] = df[\"reviewText\"].apply(clean_preprocess_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79864efc",
   "metadata": {},
   "source": [
    "#### SA Part 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b243557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1131  3636]\n",
      " [  576 14657]]\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.24      0.35      4767\n",
      "           1       0.80      0.96      0.87     15233\n",
      "\n",
      "    accuracy                           0.79     20000\n",
      "   macro avg       0.73      0.60      0.61     20000\n",
      "weighted avg       0.77      0.79      0.75     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis (nltk)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiments(text):\n",
    "    \n",
    "    score = analyzer.polarity_scores(text)\n",
    "    \n",
    "    sentiment = 1 if score[\"pos\"] > 0 else 0\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "df[\"sentiment\"] = df[\"reviewText2\"].apply(get_sentiments)\n",
    "\n",
    "# evaluation - test\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(df[\"Positive\"], df[\"sentiment\"])\n",
    "\n",
    "print (cm)\n",
    "\n",
    "cr = classification_report(df[\"Positive\"], df[\"sentiment\"])\n",
    "\n",
    "print(f\"Classification report: \\n{cr}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
